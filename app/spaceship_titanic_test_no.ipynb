{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toc_OzXYQcwH"
      },
      "source": [
        "# Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 914,
      "metadata": {
        "id": "RWIOS2y_QcwH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from  colorama  import  Fore\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar test.csv y sample_submission.csv\n",
        "df_test = pd.read_csv('../data/test.csv')\n",
        "df_submission = pd.read_csv('../data/sample_submission.csv')\n",
        "\n",
        "# Ordenar el DataFrame de test por PassengerId\n",
        "df_test_sorted = df_test.sort_values(by='PassengerId').reset_index(drop=True)\n",
        "\n",
        "# Ordenar el DataFrame de sample_submission por PassengerId\n",
        "df_submission_sorted = df_submission.sort_values(by='PassengerId').reset_index(drop=True)\n",
        "\n",
        "ids_coinciden = (df_test_sorted['PassengerId'] == df_submission_sorted['PassengerId']).all()\n",
        "\n",
        "if ids_coinciden:\n",
        "    print(\"Los PassengerId coinciden en ambos datasets después de ordenarlos.\")\n",
        "else:\n",
        "    print(\"Los PassengerId no coinciden después de ordenarlos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 916,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agregar la columna 'Transported' desde sample_submission.csv al test.csv\n",
        "df_test_sorted['Transported'] = df_submission_sorted['Transported']\n",
        "df_raw = df_test_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dePM9qXKQcwJ"
      },
      "source": [
        "# Exploración de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45TsUuwkQcwJ"
      },
      "outputs": [],
      "source": [
        "# Descripción del conjunto de datos, estándard.\n",
        "print(f\"\\n{Fore.CYAN}Número de filas y columnas:\\n{Fore.RESET}\")\n",
        "print(df_raw.shape)  # Muestra (n_filas, n_columnas)\n",
        "print(f\"\\n{Fore.CYAN}Informacion del dataset:\\n{Fore.RESET}\")\n",
        "print(df_raw.info())\n",
        "print(f\"\\n{Fore.CYAN}Columnas del dataset:\\n{Fore.RESET}\")\n",
        "print(df_raw.columns)  # Lista las columnas del dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md7i8gXBQcwJ"
      },
      "source": [
        "#### Calcular el número de nulos de cada feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXz4mT0QcwJ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Contar los nulos por variable.\n",
        "print(f\"\\n{Fore.CYAN}Valores nulos por columna:\\n{Fore.RESET}\")\n",
        "df_raw.isnull().sum()  # Suma los valores nulos por columna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Correlacionar la columna Cabin con VIP puede ser útil para imputar valores nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJv-ez3WQcwK"
      },
      "source": [
        "#### Buscar valores extraños. Para ello, ver los valores únicos en cada feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUZ2EHmTQcwK"
      },
      "outputs": [],
      "source": [
        "# Obtener un nuevo dataframe de dos columnas donde en la primera estén las features (features) y en la otra los valores únicos\n",
        "# asociados (n_values).\n",
        "\n",
        "unique_values_df = pd.DataFrame({\n",
        "    'features': df_raw.columns,\n",
        "    'n_values': [df_raw[col].nunique() for col in df_raw.columns]\n",
        "})\n",
        "\n",
        "print(f\"\\n{Fore.CYAN}Valores únicos por columna:\\n{Fore.RESET}\")\n",
        "print(unique_values_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra el número total de duplicados \n",
        "print(f\"\\n{Fore.CYAN}Número total de duplicados:\\n{Fore.RESET}\")\n",
        "print(df_raw.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Información estadística inicial\n",
        "print(f\"\\n{Fore.CYAN}Información estadística inicial:\\n{Fore.RESET}\")\n",
        "df_raw.describe() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 925,
      "metadata": {},
      "outputs": [],
      "source": [
        "#funcion para mostrar gráficos\n",
        "\n",
        "def save_and_clear_plot(file_name, output_folder=\"eda_plots\"):\n",
        "    \"\"\"\n",
        "    Saves the current plot to a specified folder and then clears the figure.\n",
        "\n",
        "    Parameters:\n",
        "    file_name (str): The name of the image file to save (with extension).\n",
        "    output_folder (str): The folder where the image will be saved (default is \"eda_plots\").\n",
        "    \"\"\"\n",
        "    # Create the folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Build the full file path\n",
        "    output_path = os.path.join(output_folder, file_name)\n",
        "\n",
        "    # Save the current plot to the specified path\n",
        "    plt.savefig(output_path)\n",
        "\n",
        "    # Show the plot (optional)\n",
        "    plt.show()\n",
        "\n",
        "    # Clear the current figure to avoid overlapping plots\n",
        "    plt.clf()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 926,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Reemplazo de Valores Vacíos por NaN\n",
        "df_modified = df_raw.replace(\"\", np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contar_interrogacion_en_columnas(df):   \n",
        "    # Iterar sobre todas las columnas del DataFrame\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object':  # Solo para columnas de tipo 'object' (categóricas)\n",
        "            # Contar cuántas veces aparece '?' en la columna\n",
        "            count = (df[column] == '?').sum()\n",
        "            print(f\"Columna '{column}' tiene {count} filas con '?'\")\n",
        "            if count == 0:\n",
        "                print(f\"{Fore.CYAN}No se realiza ninguna accion\\n{Fore.RESET}\")\n",
        "            else:\n",
        "                df_modified = df_modified.replace(\"?\", np.nan)\n",
        "\n",
        "\n",
        "# Aplicar la función al DataFrame\n",
        "print(f\"\\n{Fore.CYAN}Contar cuántas veces aparece '?':\\n{Fore.RESET}\")\n",
        "contar_interrogacion_en_columnas(df_modified)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = df_modified.columns\n",
        "n_values = [df_modified[a].unique() for a in df_modified.columns]\n",
        "\n",
        "cuenta = pd.DataFrame()\n",
        "cuenta['features'] = columns\n",
        "cuenta['n_values'] = n_values\n",
        "cuenta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar el tamaño del gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Crear un heatmap para mostrar los valores nulos\n",
        "sns.heatmap(df_modified.isnull(), cbar=False, cmap='viridis')\n",
        "\n",
        "# Título del gráfico\n",
        "plt.title('Visualización de Valores Nulos en el Dataset')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Guardar el heatmap y limpiar la figura\n",
        "save_and_clear_plot(\"heatmap_valores_nulos.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eliminando los nulos de VIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 930,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir la columna Cabin en Deck, Num, y Side\n",
        "df_modified[['Deck', 'Num', 'Side']] = df_modified['Cabin'].str.split('/', expand=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la proporción de VIPs por Deck\n",
        "print(f\"\\n{Fore.CYAN}Calcular la proporción de VIPs por Deck:\\n{Fore.RESET}\")\n",
        "vip_by_deck = pd.crosstab(df_modified['Deck'], df_modified['VIP'], normalize='index')\n",
        "print(vip_by_deck)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 932,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir las cubiertas y sus probabilidades de VIP según la distribución observada\n",
        "decks = vip_by_deck.index.tolist()\n",
        "\n",
        "# Función para imputar VIP en función de Deck\n",
        "def impute_vip_with_prob(row):\n",
        "    if pd.isnull(row['VIP']):\n",
        "        deck = row['Deck']\n",
        "        if deck in vip_by_deck.index:\n",
        "            # Extraer la probabilidad de ser VIP para la cubierta actual\n",
        "            prob_vip = vip_by_deck.loc[deck, True]\n",
        "            # Imputar VIP basado en la probabilidad observada\n",
        "            return np.random.choice([True, False], p=[prob_vip, 1 - prob_vip])\n",
        "    return row['VIP']\n",
        "\n",
        "# Aplicar la imputación de VIP basada en la cubierta (Deck)\n",
        "df_modified['VIP'] = df_modified.apply(impute_vip_with_prob, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar si quedan valores nulos en VIP\n",
        "print(f\"\\n{Fore.CYAN}Verificar si quedan valores nulos en VIP:\\n{Fore.RESET}\")\n",
        "print(df_modified['VIP'].isnull().sum())\n",
        "\n",
        "# Comprobar la distribución de VIP después de la imputación\n",
        "print(f\"\\n{Fore.CYAN}Comprobar la distribución de VIP después de la imputación:\\n{Fore.RESET}\")\n",
        "print(df_modified['VIP'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear una tabla de contingencia para contar las combinaciones de CryoSleep y VIP\n",
        "cryo_vip_table = pd.crosstab(df_modified['CryoSleep'], df_modified['VIP'])\n",
        "\n",
        "# Crear un gráfico de barras apiladas para visualizar la relación entre CryoSleep y VIP\n",
        "cryo_vip_table.plot(kind='bar', stacked=True, figsize=(8, 6))\n",
        "\n",
        "# Etiquetas y título\n",
        "plt.title('Relación entre CryoSleep y VIP')\n",
        "plt.xlabel('CryoSleep')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.legend(title='VIP', labels=['No VIP', 'VIP'])\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El gráfico sugiere que, en este conjunto de datos, el estado de criosueño no está asociado de manera notable con el hecho de ser VIP, dado que el porcentaje de pasajeros VIP en ambos grupos es muy bajo, luego, no podemos imputar los 6 restantes basandonos en si estan en criosueño o no."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_vip_por_grupo(df):\n",
        "    # Extraer el grupo del 'PassengerId'\n",
        "    df['Group'] = df['PassengerId'].str.split('_', expand=True)[0]\n",
        "\n",
        "    # Calcular la proporción de VIPs por Group\n",
        "    print(f\"\\n{Fore.CYAN}Calcular la proporción de VIPs por Group:\\n{Fore.RESET}\")\n",
        "    group_vip_counts = pd.crosstab(df['Group'], df['VIP'], normalize='index')\n",
        "    print(group_vip_counts)\n",
        "\n",
        "    # Llenar los NaN con 0 en caso de que no haya VIPs en algún grupo\n",
        "    group_vip_probs = group_vip_counts.fillna(0)\n",
        "\n",
        "    # Calcular la probabilidad global de ser VIP (en caso de grupos desconocidos)\n",
        "    global_vip_prob = df['VIP'].mean()\n",
        "\n",
        "    print(f\"\\n{Fore.CYAN}Iteramos por los valores nulos en VIP y asignamos 'False' o 'True' en función de la probabilidad:\\n{Fore.RESET}\")\n",
        "    # Iterar por los valores nulos en VIP y asignar 'False' o 'True' en función de la probabilidad\n",
        "    for index, row in df[df['VIP'].isnull()].iterrows():\n",
        "        group = row['Group']\n",
        "        \n",
        "        # Verificar si el grupo está en la tabla de probabilidades\n",
        "        if group in group_vip_probs.index:\n",
        "            # Extraer la probabilidad de VIP para el grupo\n",
        "            prob_vip = group_vip_probs.loc[group, True] if True in group_vip_probs.columns else 0\n",
        "        else:\n",
        "            # Si el grupo no está en la tabla, usamos la probabilidad global\n",
        "            prob_vip = global_vip_prob\n",
        "        \n",
        "        # Asignar True o False de manera probabilística\n",
        "        df.at[index, 'VIP'] = np.random.choice([True, False], p=[prob_vip, 1 - prob_vip])\n",
        "\n",
        "        # Mostrar el mensaje con la probabilidad y la imputación realizada\n",
        "        imputacion = 'True' if df.at[index, 'VIP'] else 'False'\n",
        "        print(f\"Pasajero en grupo {group} tiene una probabilidad de {prob_vip:.2%} de ser VIP, imputando el valor {imputacion}.\")\n",
        "    \n",
        "    # Verificar si quedan valores nulos en VIP\n",
        "    print(f\"\\n{Fore.CYAN}Verificar si quedan valores nulos en VIP:\\n{Fore.RESET}\")\n",
        "    print(df['VIP'].isnull().sum())\n",
        "\n",
        "    # Comprobar la distribución de VIP después de la imputación\n",
        "    print(f\"\\n{Fore.CYAN}Comprobar la distribución de VIP después de la imputación:\\n{Fore.RESET}\")\n",
        "    print(df['VIP'].value_counts(normalize=True))\n",
        "\n",
        "# Ejecutar la función para imputar los valores nulos en VIP\n",
        "imputar_vip_por_grupo(df_modified)\n",
        "\n",
        "\n",
        "# Verificar si quedan valores nulos en VIP\n",
        "print(f\"\\n{Fore.CYAN}Verificar si quedan valores nulos en VIP:\\n{Fore.RESET}\")\n",
        "print(df_modified['VIP'].isnull().sum())\n",
        "\n",
        "# Comprobar la distribución de VIP después de la imputación\n",
        "print(f\"\\n{Fore.CYAN}Comprobar la distribución de VIP después de la imputación:\\n{Fore.RESET}\")\n",
        "print(df_modified['VIP'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Buscar outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def numeric_column_outliers(column: str):\n",
        "    # Calcular Q1 (25%) y Q3 (75%)\n",
        "    Q1 = df_modified[column].quantile(0.25)\n",
        "    Q3 = df_modified[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Definir los límites para identificar outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filtrar los outliers\n",
        "    outliers = df_modified[(df_modified[column] < lower_bound) | \n",
        "                           (df_modified[column] > upper_bound)]\n",
        "    return outliers.shape[0]\n",
        "\n",
        "# Filtrar columnas numéricas\n",
        "numeric_columns = df_raw.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Aplicar la función a las columnas numéricas\n",
        "for column in numeric_columns:\n",
        "    print(f\"\\n{Fore.LIGHTCYAN_EX}Número de outliers en {column}:{Fore.RESET}\\n\\n{numeric_column_outliers(column)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_numeric_histograms(df):\n",
        "    # Seleccionamos todas las columnas numéricas\n",
        "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    \n",
        "    # Crear un histograma para cada columna numérica\n",
        "    for column in numeric_columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=df[column])\n",
        "        plt.title(f'Boxplot de {column}', fontsize=15)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "plot_numeric_histograms(df_modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identify_outliers(df, column):\n",
        "    # Calcular cuartiles para identificar outliers\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filtrar los outliers en esa columna\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "def analyze_outliers():\n",
        "    numeric_columns = df_modified.select_dtypes(include=['float64', 'int64']).columns\n",
        "    \n",
        "    # Iterar sobre cada columna numérica para analizar los outliers\n",
        "    for column in numeric_columns:\n",
        "        # Identificar outliers en la columna actual\n",
        "        outliers_in_column = identify_outliers(df_modified, column)\n",
        "        \n",
        "        print(f\"\\n{Fore.LIGHTCYAN_EX}Ver la proporción de `Transported` entre los outliers de {column}:\\n{Fore.RESET}\")\n",
        "        if not outliers_in_column.empty:\n",
        "            outliers_transport = outliers_in_column['Transported'].value_counts(normalize=True)\n",
        "            print(outliers_transport)\n",
        "        else:\n",
        "            print(f\"No hay outliers en {column}\")\n",
        "        \n",
        "        # Filtrar pasajeros que no son outliers en la columna actual\n",
        "        non_outliers_in_column = df_modified[~df_modified.index.isin(outliers_in_column.index)]\n",
        "\n",
        "        print(f\"\\n{Fore.LIGHTCYAN_EX}Ver la proporción de `Transported` entre los no-outliers de {column}:\\n{Fore.RESET}\")\n",
        "        non_outliers_transport = non_outliers_in_column['Transported'].value_counts(normalize=True)\n",
        "        print(non_outliers_transport)\n",
        "\n",
        "# Ejecutar la función\n",
        "analyze_outliers()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decidimos no tratar los outliers en Age e imputar los nulos con la mediana "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Crear un objeto SimpleImputer para imputar la mediana\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Aplicar la imputación a la columna 'Age'\n",
        "df_modified['Age'] = imputer.fit_transform(df_modified[['Age']])\n",
        "\n",
        "# Verificar si quedan valores nulos en Age\n",
        "print(f\"\\n{Fore.CYAN}Verificar si quedan valores nulos en Age:\\n{Fore.RESET}\")\n",
        "print(df_modified['Age'].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continuamos con el analisis de los Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar la proporción de VIPs entre los outliers y no-outliers de cada columna de gastos\n",
        "def analyze_vip_vs_outliers(column):\n",
        "    # Identificar los outliers en la columna actual\n",
        "    outliers_in_column = identify_outliers(df_modified, column)\n",
        "    \n",
        "    # Proporción de pasajeros VIP entre los outliers\n",
        "    vip_outliers = outliers_in_column['VIP'].value_counts(normalize=True)\n",
        "\n",
        "    print(f\"\\n{Fore.LIGHTCYAN_EX}Proporción de VIPs entre los outliers de {column}:{Fore.RESET}\\n\\n{vip_outliers}\\n\")\n",
        "    \n",
        "    # Proporción de pasajeros VIP entre los no-outliers\n",
        "    non_outliers_in_column = df_modified[~df_modified.index.isin(outliers_in_column.index)]\n",
        "    vip_non_outliers = non_outliers_in_column['VIP'].value_counts(normalize=True)\n",
        "    print(f\"\\n{Fore.LIGHTCYAN_EX}Proporción de VIPs entre los no-outliers de {column}:{Fore.RESET}\\n\\n{vip_non_outliers}\\n\")\n",
        "\n",
        "# Aplicar el análisis a las columnas de gastos\n",
        "analyze_vip_vs_outliers('RoomService')\n",
        "analyze_vip_vs_outliers('ShoppingMall')\n",
        "analyze_vip_vs_outliers('FoodCourt')\n",
        "analyze_vip_vs_outliers('Spa')\n",
        "analyze_vip_vs_outliers('VRDeck')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la correlación entre VIP y Transported\n",
        "correlation = df_modified[['VIP', 'Transported']].corr()\n",
        "\n",
        "# Mostrar la correlación entre VIP y Transported\n",
        "correlation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El hecho de que la correlación entre VIP y Transported sea tan débil (-0.03608) sugiere que el estatus de ser VIP no tiene una relación significativa con el resultado de haber sido transportado. Esto plantea la hipótesis de que los outliers en los gastos de los pasajeros VIP podrían no ser muy relevantes para predecir si un pasajero fue transportado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que VIP en sí no tiene una gran influencia, los outliers en gastos altos pueden estar introduciendo ruido en lugar de contribuir a una mejor predicción, por lo que decidimos aplicar transformaciones para reducir la influencia de los valores extremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 942,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar transformación logarítmica a los gastos\n",
        "df_modified['RoomService_log'] = np.log1p(df_modified['RoomService'])\n",
        "df_modified['ShoppingMall_log'] = np.log1p(df_modified['ShoppingMall'])\n",
        "df_modified['FoodCourt_log'] = np.log1p(df_modified['FoodCourt'])\n",
        "df_modified['Spa_log'] = np.log1p(df_modified['Spa'])\n",
        "df_modified['VRDeck_log'] = np.log1p(df_modified['VRDeck'])\n",
        "\n",
        "# Eliminar las columnas originales después de la transformación logarítmica\n",
        "df_modified.drop(['RoomService', 'ShoppingMall', 'FoodCourt', 'Spa', 'VRDeck'], axis=1, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_numeric_histograms(df_modified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputamos los valores nulos con la mediana en 'RoomService_log', 'ShoppingMall_log', 'FoodCourt_log', 'Spa_log', 'VRDeck_log'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 944,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Crear un imputador usando la mediana\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# Seleccionar las columnas logarítmicas\n",
        "log_columns = ['RoomService_log', 'ShoppingMall_log', 'FoodCourt_log', 'Spa_log', 'VRDeck_log']\n",
        "\n",
        "# Aplicar la imputación con la mediana\n",
        "df_modified[log_columns] = imputer.fit_transform(df_modified[log_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar si quedan valores nulos en las columnas logarítmicas\n",
        "print(f\"\\n{Fore.LIGHTCYAN_EX}Verificar si quedan valores nulos en las columnas logarítmicas:{Fore.RESET}\\n\")\n",
        "print(df_modified[log_columns].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un gráfico de calor (heatmap) para visualizar los valores nulos restantes en el DataFrame\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_modified.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Visualización de Valores Nulos Restantes en el Dataset')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aun así siguen quedando bastantes valores nulos  que debemos tratar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 947,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer el apellido del nombre (suponiendo que el apellido es la última palabra del nombre)\n",
        "df_modified['LastName'] = df_modified['Name'].apply(lambda x: x.split()[-1] if pd.notnull(x) else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_lastname_por_grupo(df):\n",
        "    # Verificar cuántos valores nulos hay inicialmente en LastName\n",
        "    print(f\"Valores nulos iniciales en LastName: {df['LastName'].isnull().sum()}\")\n",
        "\n",
        "    # Filtrar los grupos donde LastName no es nulo para tener apellidos de referencia\n",
        "    groups_with_lastname = df.dropna(subset=['LastName'])\n",
        "\n",
        "    def imputar_fila(row):\n",
        "        # Imputar solo si LastName es nulo\n",
        "        if pd.isnull(row['LastName']):\n",
        "            group = row['Group']\n",
        "            \n",
        "            # Filtrar el grupo actual para buscar apellidos\n",
        "            potential_lastnames = groups_with_lastname[groups_with_lastname['Group'] == group]['LastName']\n",
        "            \n",
        "            # Si hay apellidos disponibles en el grupo, imputar el más frecuente\n",
        "            if len(potential_lastnames) > 0:\n",
        "                imputed_lastname = potential_lastnames.mode()[0]\n",
        "                print(f\"Imputando apellido {imputed_lastname} para el grupo {group}.\")\n",
        "                return imputed_lastname\n",
        "        \n",
        "        return row['LastName']  # Dejar sin cambios si no se puede imputar\n",
        "\n",
        "    # Aplicar la imputación fila por fila\n",
        "    df['LastName'] = df.apply(imputar_fila, axis=1)\n",
        "\n",
        "    # Verificar cuántos valores nulos quedan después de la imputación\n",
        "    print(f\"Valores nulos restantes en LastName después de la imputación: {df['LastName'].isnull().sum()}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df_modified = imputar_lastname_por_grupo(df_modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 949,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Agrupar por 'Cabin' y contar cuántos pasajeros tienen el mismo apellido en la misma cabina\n",
        "df_modified['MismoGrupo'] = df_modified.groupby('Cabin')['Group'].transform(lambda x: x.duplicated(keep=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paso 1: Contar la frecuencia de cada apellido en el dataset\n",
        "apellido_counts = df_modified['LastName'].value_counts()\n",
        "\n",
        "# Paso 2: Filtrar los pasajeros cuyos apellidos aparecen más de una vez\n",
        "passengers_with_shared_lastname = df_modified[df_modified['LastName'].isin(apellido_counts[apellido_counts > 1].index)]\n",
        "\n",
        "# Filtrar los pasajeros que comparten apellido y grupo\n",
        "passengers_with_shared_lastname_group_and_planet = passengers_with_shared_lastname.groupby(['LastName', 'Group']).filter(lambda x: len(x) > 1)\n",
        "\n",
        "# Paso 4: Calcular cuántos de estos pasajeros también comparten la misma cabina\n",
        "shared_cabin = passengers_with_shared_lastname_group_and_planet.groupby(['LastName', 'Group', 'Cabin']).size().reset_index(name='count')\n",
        "\n",
        "# Contar cuántos grupos comparten apellido, grupo y HomePlanet y también cabina\n",
        "shared_cabin_counts = shared_cabin[shared_cabin['count'] > 1]\n",
        "\n",
        "# Paso 5: Calcular la proporción de personas que comparten apellido, grupo, y además comparten cabina\n",
        "total_shared_lastname_group_and_planet = len(passengers_with_shared_lastname_group_and_planet)\n",
        "total_shared_cabin = shared_cabin_counts['count'].sum()\n",
        "\n",
        "# Calcular la probabilidad\n",
        "probability_shared_cabin = total_shared_cabin / total_shared_lastname_group_and_planet\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(f\"\\n{Fore.LIGHTCYAN_EX}La probabilidad de que una persona que comparte apellido y grupo también esté en la misma cabina es:{Fore.RESET} {probability_shared_cabin:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 951,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_cabin_por_probabilidad(df):\n",
        "    # Paso 1: Filtrar por apellidos compartidos\n",
        "    apellido_counts = df['LastName'].value_counts()\n",
        "    passengers_with_shared_lastname = df[df['LastName'].isin(apellido_counts[apellido_counts > 1].index)]\n",
        "\n",
        "    # Pasajeros con cabina asignada\n",
        "    passengers_with_cabin = passengers_with_shared_lastname.dropna(subset=['Cabin'])\n",
        "\n",
        "    def imputar_fila(row):\n",
        "        # Imputación basada en apellido y grupo\n",
        "        if pd.isnull(row['Cabin']):\n",
        "            group = row['Group']\n",
        "            lastname = row['LastName']\n",
        "            \n",
        "            # Filtrar compañeros de cabina basados en apellido y grupo\n",
        "            potential_cabin_mates = passengers_with_cabin[(passengers_with_cabin['LastName'] == lastname) & \n",
        "                                                          (passengers_with_cabin['Group'] == group)]\n",
        "            \n",
        "            # Si hay coincidencias en apellido y grupo, imputar la cabina más común\n",
        "            if len(potential_cabin_mates) > 0:\n",
        "                imputed_cabin = potential_cabin_mates['Cabin'].mode()[0]\n",
        "                print(f\"Lo más probable es que {lastname} se encuentre en la cabina {imputed_cabin}, imputando el valor.\")\n",
        "                return imputed_cabin\n",
        "\n",
        "        # Si aún queda nulo, buscar compañeros de cabina solo basados en el grupo\n",
        "        if pd.isnull(row['Cabin']):\n",
        "            potential_cabin_mates_group = df[(df['Group'] == row['Group']) & df['Cabin'].notnull()]\n",
        "            \n",
        "            # Si hay coincidencias en el grupo, imputar la cabina más común\n",
        "            if len(potential_cabin_mates_group) > 0:\n",
        "                imputed_cabin_group = potential_cabin_mates_group['Cabin'].mode()[0]\n",
        "                print(f\"Lo más probable es que alguien en el grupo {group} se encuentre en la cabina {imputed_cabin_group}, imputando el valor.\")\n",
        "                return imputed_cabin_group\n",
        "        \n",
        "        return row['Cabin']  # Dejar sin cambios si no se puede imputar\n",
        "\n",
        "    # Aplicar la imputación fila por fila\n",
        "    df['Cabin'] = df.apply(imputar_fila, axis=1)\n",
        "\n",
        "    print(f\"\\n{Fore.LIGHTCYAN_EX}Imputación de valores nulos en 'Cabin' completada.{Fore.RESET}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vamos a eliminar los nulos de Destination antes de seguir con las cabinas "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copiamos el DataFrame para no afectar el original\n",
        "df_encoded = df_modified.copy()\n",
        "\n",
        "# Identificamos las columnas categóricas\n",
        "categorical_cols = df_encoded.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Aplicamos LabelEncoder a las columnas categóricas\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    df_encoded[col] = df_encoded[col].astype(str).fillna('missing')  # Llenamos los nulos con un valor temporal\n",
        "    df_encoded[col] = label_encoder.fit_transform(df_encoded[col])\n",
        "\n",
        "# Paso 1: Calcular la matriz de correlación\n",
        "correlacion = df_encoded.corr()\n",
        "\n",
        "# Paso 2: Generar el mapa de calor\n",
        "plt.figure(figsize=(25, 20))  # Ajustar el tamaño de la figura si es necesario\n",
        "sns.heatmap(correlacion, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.title(\"Mapa de correlación de todas las variables (incluyendo categóricas)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vemos la distribucion de Destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 953,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calcular_probabilidad(df, item, column):\n",
        "    \"\"\"\n",
        "    Calcula la probabilidad de cada item para cada column en el DataFrame.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): El DataFrame que contiene las columnas 'column' y 'item'.\n",
        "    \n",
        "    Returns:\n",
        "    pandas.DataFrame: Un DataFrame que muestra las probabilidades de cada 'item' para cada 'column'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calcular las frecuencias relativas de 'item' para cada 'HomePlanet'\n",
        "    probabilidad_condicional = df.groupby(column)[item].value_counts(normalize=True).unstack(fill_value=0)\n",
        "    \n",
        "    return probabilidad_condicional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "columns = ['HomePlanet','Transported','CryoSleep','VIP','Deck']\n",
        "        \n",
        "for column in columns:\n",
        "    # Comparar la distribución \n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.countplot(data=df_modified, x=column, hue='Destination')\n",
        "    plt.title(f'Distribución de Destination por {column}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deck parece tener un poco mas de importancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vamos a escalar las variables numericas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 955,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Instanciar el escalador\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Seleccionar las columnas de gastos\n",
        "gasto_columns = ['RoomService_log', 'ShoppingMall_log', 'FoodCourt_log', 'Spa_log', 'VRDeck_log']\n",
        "\n",
        "# Aplicar la normalización Min-Max\n",
        "df_modified[gasto_columns] = scaler.fit_transform(df_modified[gasto_columns])\n",
        "\n",
        "# Normalización Min-Max de Age\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalizar 'Age'\n",
        "df_modified['Age'] = scaler.fit_transform(df_modified[['Age']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_nulos_por_probabilidad(df, item, column):\n",
        "    \"\"\"\n",
        "    Imputa los valores nulos de 'item' en función de las probabilidades condicionales basadas en 'column'.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): El DataFrame que contiene las columnas 'item' y 'column'.\n",
        "    item (str): La columna que contiene los valores nulos a imputar (ej. 'HomePlanet').\n",
        "    column (str): La columna que se usa como referencia (ej. 'Deck').\n",
        "    probabilidades (pandas.DataFrame): Un DataFrame de probabilidades condicionales calculado previamente.\n",
        "    \n",
        "    Returns:\n",
        "    pandas.DataFrame: El DataFrame con los valores nulos imputados.\n",
        "    \"\"\"\n",
        "\n",
        "    probabilidades = calcular_probabilidad(df_modified, item=item, column=column)\n",
        "    \n",
        "    # Iterar sobre las filas donde 'item' es nulo\n",
        "    for idx, row in df[df[item].isnull()].iterrows():\n",
        "        valor_referencia = row[column]\n",
        "        \n",
        "        # Verificar que el valor de 'column' no sea nulo\n",
        "        if pd.notnull(valor_referencia):\n",
        "            try:\n",
        "                # Obtener las probabilidades del 'item' en función del valor de 'column'\n",
        "                probas = probabilidades.loc[valor_referencia]\n",
        "                \n",
        "                # Imputar el valor de 'item' basado en una elección aleatoria ponderada por las probabilidades\n",
        "                imputed_value = np.random.choice(probas.index, p=probas.values)\n",
        "                df.at[idx, item] = imputed_value\n",
        "                \n",
        "                print(f\"Imputando {item}: {imputed_value} para {column}: {valor_referencia}\")\n",
        "            except KeyError:\n",
        "                # Si no se encuentran probabilidades para la referencia, omitir la imputación\n",
        "                print(f\"No se encontraron probabilidades para {column}: {valor_referencia}\")\n",
        "                pass\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Usar la función para imputar los valores nulos en HomePlanet\n",
        "df_modified = imputar_nulos_por_probabilidad(df_modified, item='Destination', column='Deck')\n",
        "\n",
        "# Contar los nulos restantes en la columna 'Destination'\n",
        "nulos_destination = df_modified['Destination'].isnull().sum()\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(f\"\\n{Fore.CYAN}Nulos restantes en 'Destination': {nulos_destination}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 709,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def imputar_destination_por_probabilidad(df):\n",
        "#     # Primero, calculamos las probabilidades condicionales en base a las combinaciones de las variables más relevantes\n",
        "#     probabilidad_condicional = df.dropna(subset=['Destination']).groupby(['HomePlanet', 'Transported', 'CryoSleep', 'VIP', 'Deck'])['Destination'].value_counts(normalize=True).unstack(fill_value=0)\n",
        "    \n",
        "#     # Iteramos sobre los valores nulos de Destination\n",
        "#     for idx, row in df[df['Destination'].isnull()].iterrows():\n",
        "#         homeplanet = row['HomePlanet']\n",
        "#         transported = row['Transported']\n",
        "#         cryosleep = row['CryoSleep']\n",
        "#         vip = row['VIP']\n",
        "#         deck = row['Deck']\n",
        "        \n",
        "#         # Verificamos si existen probabilidades calculadas para esta combinación\n",
        "#         try:\n",
        "#             probas = probabilidad_condicional.loc[(homeplanet, transported, cryosleep, vip, deck)]\n",
        "#             # Si existen probabilidades, elegimos el destino en función de ellas\n",
        "#             imputed_destination = np.random.choice(probas.index, p=probas.values)\n",
        "#             df.at[idx, 'Destination'] = imputed_destination\n",
        "#             print(f\"Pasajero en HomePlanet: {homeplanet}, Transported: {transported}, CryoSleep: {cryosleep}, VIP: {vip}, Deck: {deck} - Imputando Destination: {imputed_destination}\")\n",
        "#         except KeyError:\n",
        "#             # Si no hay suficiente información para la combinación, lo dejamos nulo\n",
        "#             print(f\"No se encontró suficiente información para imputar el Destination del pasajero con HomePlanet: {homeplanet}, Transported: {transported}, CryoSleep: {cryosleep}, VIP: {vip}, Deck: {deck}.\")\n",
        "#             pass\n",
        "\n",
        "#     return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 710,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_homeplanet_por_probabilidad(df):\n",
        "    # Primero, calculamos las probabilidades condicionales en base a las combinaciones de las variables más relevantes\n",
        "    probabilidad_condicional = df.dropna(subset=['HomePlanet']).groupby(['Destination', 'Transported', 'CryoSleep', 'VIP', 'Deck'])['HomePlanet'].value_counts(normalize=True).unstack(fill_value=0)\n",
        "    \n",
        "    # Iteramos sobre los valores nulos de HomePlanet\n",
        "    for idx, row in df[df['HomePlanet'].isnull()].iterrows():\n",
        "        destination = row['Destination']\n",
        "        transported = row['Transported']\n",
        "        cryosleep = row['CryoSleep']\n",
        "        vip = row['VIP']\n",
        "        deck = row['Deck']\n",
        "        \n",
        "        # Verificamos si existen probabilidades calculadas para esta combinación\n",
        "        try:\n",
        "            probas = probabilidad_condicional.loc[(destination, transported, cryosleep, vip, deck)]\n",
        "            # Si existen probabilidades, elegimos el HomePlanet en función de ellas\n",
        "            imputed_homeplanet = np.random.choice(probas.index, p=probas.values)\n",
        "            df.at[idx, 'HomePlanet'] = imputed_homeplanet\n",
        "            print(f\"Pasajero en Destination: {destination}, Transported: {transported}, CryoSleep: {cryosleep}, VIP: {vip}, Deck: {deck} - Imputando HomePlanet: {imputed_homeplanet}\")\n",
        "        except KeyError:\n",
        "            # Si no hay suficiente información para la combinación, lo dejamos nulo\n",
        "            print(f\"No se encontró suficiente información para imputar el HomePlanet del pasajero con Destination: {destination}, Transported: {transported}, CryoSleep: {cryosleep}, VIP: {vip}, Deck: {deck}.\")\n",
        "            pass\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 711,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_destination_homeplanet(df):\n",
        "    # Imputar valores faltantes en 'Destination'\n",
        "    for index, row in df[df['Destination'].isnull()].iterrows():\n",
        "        # Variables condicionales: HomePlanet, VIP, CryoSleep, Transported, Deck\n",
        "        homeplanet = row['HomePlanet']\n",
        "        vip = row['VIP']\n",
        "        cryosleep = row['CryoSleep']\n",
        "        transported = row['Transported']\n",
        "        deck = row['Deck']\n",
        "        \n",
        "        # Filtrar filas con condiciones similares\n",
        "        similar_rows = df[(df['HomePlanet'] == homeplanet) &\n",
        "                          (df['VIP'] == vip) &\n",
        "                          (df['CryoSleep'] == cryosleep) &\n",
        "                          (df['Transported'] == transported) &\n",
        "                          (df['Deck'] == deck) &\n",
        "                          df['Destination'].notnull()]\n",
        "        \n",
        "        if len(similar_rows) > 0:\n",
        "            # Imputar la moda (valor más frecuente) de 'Destination'\n",
        "            mode_destination = similar_rows['Destination'].mode()[0]\n",
        "            df.at[index, 'Destination'] = mode_destination\n",
        "        else:\n",
        "            # Si no hay coincidencias, imputar aleatoriamente en base a la probabilidad\n",
        "            df.at[index, 'Destination'] = np.random.choice(df['Destination'].dropna().unique(), p=[0.6, 0.3, 0.1])  # Ajustar probabilidades\n",
        "\n",
        "    # Imputar valores faltantes en 'HomePlanet'\n",
        "    for index, row in df[df['HomePlanet'].isnull()].iterrows():\n",
        "        # Variables condicionales: Destination, VIP, CryoSleep, Transported, Deck\n",
        "        destination = row['Destination']\n",
        "        vip = row['VIP']\n",
        "        cryosleep = row['CryoSleep']\n",
        "        transported = row['Transported']\n",
        "        deck = row['Deck']\n",
        "        \n",
        "        # Filtrar filas con condiciones similares\n",
        "        similar_rows = df[(df['Destination'] == destination) &\n",
        "                          (df['VIP'] == vip) &\n",
        "                          (df['CryoSleep'] == cryosleep) &\n",
        "                          (df['Transported'] == transported) &\n",
        "                          (df['Deck'] == deck) &\n",
        "                          df['HomePlanet'].notnull()]\n",
        "        \n",
        "        if len(similar_rows) > 0:\n",
        "            # Imputar la moda (valor más frecuente) de 'HomePlanet'\n",
        "            mode_homeplanet = similar_rows['HomePlanet'].mode()[0]\n",
        "            df.at[index, 'HomePlanet'] = mode_homeplanet\n",
        "        else:\n",
        "            # Si no hay coincidencias, imputar aleatoriamente en base a la probabilidad\n",
        "            df.at[index, 'HomePlanet'] = np.random.choice(df['HomePlanet'].dropna().unique(), p=[0.5, 0.4, 0.1])  # Ajustar probabilidades\n",
        "    \n",
        "    print(f\"{Fore.CYAN}Imputación completada.{Fore.RESET}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 712,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_cryosleep_por_gastos(df):\n",
        "    # Iterar sobre las filas con valores nulos en CryoSleep\n",
        "    for index, row in df[df['CryoSleep'].isnull()].iterrows():\n",
        "        # Sumar los gastos del pasajero\n",
        "        gastos_total = row[['RoomService_log', 'FoodCourt_log', 'ShoppingMall_log', 'Spa_log', 'VRDeck_log']].sum()\n",
        "        \n",
        "        # Si los gastos son 0, imputar CryoSleep como True\n",
        "        if gastos_total == 0:\n",
        "            df.at[index, 'CryoSleep'] = True\n",
        "        else:\n",
        "            # Imputar CryoSleep en función de la cabina si otros pasajeros en la misma cabina están en CryoSleep\n",
        "            cabina = row['Cabin']\n",
        "            if pd.notnull(cabina):\n",
        "                # Buscar otros pasajeros en la misma cabina\n",
        "                pasajeros_en_cabina = df[df['Cabin'] == cabina]['CryoSleep']\n",
        "                if pasajeros_en_cabina.notnull().any():\n",
        "                    # Si al menos un pasajero tiene CryoSleep imputado, asignar el valor más común (moda)\n",
        "                    df.at[index, 'CryoSleep'] = pasajeros_en_cabina.mode()[0]\n",
        "                else:\n",
        "                    # Si no hay pasajeros en CryoSleep que compartan cabina, buscar por otras variables\n",
        "                    destination = row['Destination']\n",
        "                    homeplanet = row['HomePlanet']\n",
        "                    vip = row['VIP']\n",
        "                    transported = row['Transported']\n",
        "                    \n",
        "                    # Filtrar filas con condiciones similares\n",
        "                    similar_rows = df[(df['Destination'] == destination) &\n",
        "                                      (df['HomePlanet'] == homeplanet) &\n",
        "                                      (df['VIP'] == vip) &\n",
        "                                      (df['Transported'] == transported) &\n",
        "                                      df['CryoSleep'].notnull()]\n",
        "                    \n",
        "                    if len(similar_rows) > 0:\n",
        "                        # Imputar la moda (valor más frecuente) de 'CryoSleep'\n",
        "                        df.at[index, 'CryoSleep'] = similar_rows['CryoSleep'].mode()[0]\n",
        "                        \n",
        "    print(f\"\\n{Fore.CYAN}Imputación de CryoSleep en función de gastos, cabina, y otras variables completada.{Fore.RESET}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seguimos tratando de eliminar los nulos de Cabin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 713,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_modified.drop(['Deck', 'Num', 'Side','MismoGrupo'], axis=1, inplace=True)\n",
        "\n",
        "# Separar la columna 'Cabin' en 'Deck', 'Num' y 'Side'\n",
        "df_modified[['Deck', 'Num', 'Side']] = df_modified['Cabin'].str.split('/', expand=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir una función que itere a través de las columnas y genere gráficos de frecuencia\n",
        "def plot_frequency_by_deck(df):\n",
        "    columns_to_plot = ['VIP', 'Destination', 'HomePlanet', 'CryoSleep', 'Side']\n",
        "\n",
        "    for column in columns_to_plot:\n",
        "        crosstab_vip = pd.crosstab(df['Deck'], df[column])\n",
        "        crosstab_vip.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "        plt.title(f\"Gráfico de Frecuencia entre 'Deck' y '{column}'\")\n",
        "        plt.xlabel('Deck')\n",
        "        plt.ylabel('Frecuencia')\n",
        "        plt.legend(title=column)\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()  # Asegurar que el gráfico no se recorte\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Ejecutar la función para generar los gráficos de frecuencia entre 'Deck' y las demás columnas\n",
        "plot_frequency_by_deck(df_modified)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calcular el gasto total sumando las columnas de los diferentes tipos de gastos\n",
        "df_modified['Total_Gasto'] = df_modified[['RoomService_log', 'ShoppingMall_log', 'FoodCourt_log', 'Spa_log', 'VRDeck_log']].sum(axis=1)\n",
        "\n",
        "# Crear un boxplot para visualizar la distribución de los gastos por Deck\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Deck', y='Total_Gasto', data=df_modified)\n",
        "plt.title('Distribución del Gasto Total por Deck')\n",
        "plt.xlabel('Deck')\n",
        "plt.ylabel('Total Gasto')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imputamos Deck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 716,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imputar_deck_mejorado(df):\n",
        "    for index, row in df[df['Deck'].isnull()].iterrows():\n",
        "        \n",
        "        # Prioridad 1: Mismo grupo\n",
        "        if not pd.isnull(row['Group']):\n",
        "            group_mates = df[(df['Group'] == row['Group']) & (df['Deck'].notnull())]\n",
        "            if not group_mates.empty:\n",
        "                df.at[index, 'Deck'] = group_mates['Deck'].mode()[0]  # Imputar el deck más común del grupo\n",
        "                continue\n",
        "        \n",
        "        # Prioridad 2: Gastos de lujo\n",
        "        if row['RoomService_log'] > 6 or row['FoodCourt_log'] > 6 or row['Spa_log'] > 6:\n",
        "            df.at[index, 'Deck'] = np.random.choice(['A', 'B', 'C'], p=[0.4, 0.3, 0.3])\n",
        "            continue\n",
        "        \n",
        "        # Prioridad 3: Relación con HomePlanet y Destination\n",
        "        if row['HomePlanet'] == 'Earth':\n",
        "            df.at[index, 'Deck'] = np.random.choice(['F', 'G'], p=[0.4, 0.6])\n",
        "        elif row['HomePlanet'] == 'Europa':\n",
        "            df.at[index, 'Deck'] = np.random.choice(['B', 'C', 'D'], p=[0.3, 0.4, 0.3])\n",
        "        elif row['HomePlanet'] == 'Mars':\n",
        "            df.at[index, 'Deck'] = np.random.choice(['F', 'E'], p=[0.5, 0.5])\n",
        "        \n",
        "        # Prioridad 4: Transported y CryoSleep\n",
        "        if row['CryoSleep'] == True:\n",
        "            df.at[index, 'Deck'] = np.random.choice(['F', 'G'], p=[0.5, 0.5])\n",
        "        elif row['Transported'] == True:\n",
        "            df.at[index, 'Deck'] = np.random.choice(['A', 'B', 'C', 'D'], p=[0.25, 0.25, 0.25, 0.25])\n",
        "\n",
        "    print(f\"\\n{Fore.CYAN}Imputación de valores nulos en 'Deck' completada.{Fore.RESET}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputamos resto de valores nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def realizar_imputaciones(df):\n",
        "    # Inicializar valores de nulos en cada columna\n",
        "    nulos_destination_anterior = df['Destination'].isnull().sum()\n",
        "    nulos_homeplanet_anterior = df['HomePlanet'].isnull().sum()\n",
        "    nulos_cryosleep_anterior = df['CryoSleep'].isnull().sum()\n",
        "    nulos_cabin_anterior = df['Cabin'].isnull().sum()\n",
        "\n",
        "    # Bucle para realizar imputaciones hasta que los valores nulos dejen de cambiar\n",
        "    while True:\n",
        "        # Imputar Destination\n",
        "        df = imputar_destination_por_probabilidad(df)\n",
        "        # if df['Destination'].isnull().sum() <= 8:\n",
        "        #     df = df.dropna(subset=['Destination'])\n",
        "        #     print(f\"{Fore.MAGENTA}Eliminados nulos inimputables en Destination:{Fore.RESET}\")\n",
        "\n",
        "        nulos_destination_actual = df['Destination'].isnull().sum()\n",
        "        print(f\"{Fore.LIGHTCYAN_EX}Valores nulos restantes en Destination:{Fore.RESET} {nulos_destination_actual}\\n\")\n",
        "\n",
        "        # Imputar HomePlanet\n",
        "        df = imputar_homeplanet_por_probabilidad(df)\n",
        "        if df['HomePlanet'].isnull().sum() <= 8:\n",
        "            df = df.dropna(subset=['HomePlanet'])\n",
        "            print(f\"{Fore.MAGENTA}Eliminados nulos inimputables en HomePlanet:{Fore.RESET}\")\n",
        "\n",
        "        nulos_homeplanet_actual = df['HomePlanet'].isnull().sum()\n",
        "        print(f\"{Fore.LIGHTCYAN_EX}Valores nulos restantes en HomePlanet:{Fore.RESET} {nulos_homeplanet_actual}\\n\")\n",
        "\n",
        "        # Imputar CryoSleep\n",
        "        df = imputar_cryosleep_por_gastos(df)\n",
        "        if df['CryoSleep'].isnull().sum() <= 8:\n",
        "            df = df.dropna(subset=['CryoSleep'])\n",
        "            print(f\"{Fore.MAGENTA}Eliminados nulos inimputables en CryoSleep:{Fore.RESET}\")\n",
        "\n",
        "        nulos_cryosleep_actual = df['CryoSleep'].isnull().sum()\n",
        "        print(f\"{Fore.LIGHTCYAN_EX}Valores nulos restantes en CryoSleep:{Fore.RESET} {nulos_cryosleep_actual}\\n\")\n",
        "\n",
        "        # Imputar Cabin\n",
        "        print(f\"\\n{Fore.LIGHTCYAN_EX}Imputar Cabin por probabilidad de estar en la misma cabina teniendo el mismo apellido:{Fore.RESET} {probability_shared_cabin:.2%}\")\n",
        "        df = imputar_cabin_por_probabilidad(df)\n",
        "        nulos_cabin_actual = df['Cabin'].isnull().sum()\n",
        "        print(f\"{Fore.LIGHTCYAN_EX}Valores nulos restantes en Cabin:{Fore.RESET} {nulos_cabin_actual}\\n\")\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        # Condición de parada: si los valores nulos no cambian entre iteraciones, se rompe el bucle\n",
        "        if (nulos_destination_actual == nulos_destination_anterior and\n",
        "            nulos_homeplanet_actual == nulos_homeplanet_anterior and\n",
        "            nulos_cryosleep_actual == nulos_cryosleep_anterior and\n",
        "            nulos_cabin_actual == nulos_cabin_anterior):\n",
        "            break\n",
        "\n",
        "        # Actualizar los valores anteriores para la siguiente iteración\n",
        "        nulos_destination_anterior = nulos_destination_actual\n",
        "        nulos_homeplanet_anterior = nulos_homeplanet_actual\n",
        "        nulos_cryosleep_anterior = nulos_cryosleep_actual\n",
        "        nulos_cabin_anterior = nulos_cabin_actual\n",
        "\n",
        "    print(f\"{Fore.GREEN}Imputación completa. No hay más cambios en los valores nulos.{Fore.RESET}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df_modified = realizar_imputaciones(df_modified)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 718,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_modified.drop(['Deck', 'Num', 'Side'], axis=1, inplace=True)\n",
        "\n",
        "# Separar la columna 'Cabin' en 'Deck', 'Num' y 'Side'\n",
        "df_modified[['Deck', 'Num', 'Side']] = df_modified['Cabin'].str.split('/', expand=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imputar Deck\n",
        "df_modified = imputar_deck_mejorado(df_modified)\n",
        "nulos_deck_actual = df_modified['Deck'].isnull().sum()\n",
        "print(f\"{Fore.LIGHTCYAN_EX}Valores nulos restantes en Deck:{Fore.RESET} {nulos_deck_actual}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Crear un gráfico de calor (heatmap) para visualizar los valores nulos restantes en el DataFrame\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_modified.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Visualización de Valores Nulos Restantes en el Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar los nulos por variable.\n",
        "print(f\"\\n{Fore.CYAN}Valores nulos por columna:\\n{Fore.RESET}\")\n",
        "df_modified.isnull().sum()  # Suma los valores nulos por columna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 722,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_modified = df_modified.sort_values(['Deck','Group'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Revisar la distribución de valores en la tabla sin normalización\n",
        "contingency_table = pd.crosstab(df_modified['Deck'], df_modified['Transported'])\n",
        "print(contingency_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_modified['Transported'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear una tabla de contingencia para Deck y Transported\n",
        "contingency_table = pd.crosstab(df_modified['Deck'], df_modified['Transported'], normalize='index')\n",
        "print(contingency_table)\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Realizar el test de Chi-cuadrado\n",
        "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df_modified['Deck'], df_modified['Transported']))\n",
        "\n",
        "# Mostrar el valor de p para ver si la relación es significativa\n",
        "print(f\"Valor de p: {p}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El valor de p es un número extremadamente pequeño. Esto significa que la probabilidad de que la relación observada entre Deck y Transported sea debida al azar es casi inexistente.\n",
        "\n",
        "**Interpretación:**\n",
        "p = 6.349575583001859e-78 es mucho menor que el umbral común de 0.05. Esto significa que hay una relación estadísticamente significativa entre la cubierta (Deck) en la que viaja un pasajero y si fue transportado (Transported) o no.\n",
        "\n",
        "Dado que este valor es tan pequeño, puedes concluir con mucha confianza que la cubierta en la que viaja un pasajero tiene una relación importante con la probabilidad de que haya sido transportado a otra dimensión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def side_trans(df):    \n",
        "    # Convertir 'Side' a valores numéricos\n",
        "    df['Side_numeric'] = df['Side'].map({'P': 1, 'S': 0})  # Suponiendo 'P' = 1, 'S' = 0\n",
        "\n",
        "    # Verificar si la columna 'Transported' está en formato adecuado (True/False)\n",
        "    df['Transported_numeric'] = df['Transported'].map({True: 1, False: 0})\n",
        "\n",
        "    # Calcular la correlación entre 'Side_numeric' y 'Transported_numeric'\n",
        "    correlation_side_transportation = df['Side_numeric'].corr(df['Transported_numeric'])\n",
        "\n",
        "    print(correlation_side_transportation)\n",
        "\n",
        "side_trans(df_modified)\n",
        "\n",
        "# Eliminar la columna 'Side'\n",
        "df_modified = df_modified.drop(columns=['Side','Side_numeric'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Valor cercano a 0: Esto indica que la relación entre Side (lado del barco) y Transported (si fue transportado o no) es muy débil. En otras palabras, el lado del barco en el que un pasajero estaba ubicado no parece tener un impacto significativo sobre si fue transportado o no.\n",
        "\n",
        "Con lo cual decidimos eliminar la columna para no meter ruido al modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def num_trans(df):    \n",
        "    # Convertir 'Num' a valores numéricos enteros\n",
        "    df['Num'] = pd.to_numeric(df['Num'], errors='coerce')  # Convertir a numérico, manejando nulos\n",
        "\n",
        "    # Verificar si la columna 'Transported' está en formato adecuado (True/False)\n",
        "    df['Transported_numeric'] = df['Transported'].map({True: 1, False: 0})\n",
        "\n",
        "    # Calcular la correlación entre 'Num' y 'Transported_numeric'\n",
        "    correlation_num_transportation = df['Num'].corr(df['Transported_numeric'])\n",
        "\n",
        "    # Imprimir la correlación\n",
        "    print(f\"Correlación entre 'Num' y 'Transported': {correlation_num_transportation}\")\n",
        "\n",
        "# Ejecutar la función con el DataFrame modificado\n",
        "num_trans(df_modified)\n",
        "\n",
        "# Eliminar la columna 'Num'\n",
        "df_modified = df_modified.drop(columns=['Num','Transported_numeric'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No parece haber una correlación fuerte entre el número de cabina (Num) y el transporte (Transported), lo que sugiere que el número de cabina no es un factor determinante para este resultado.\n",
        "Decidimos tambien eliminar esta columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 728,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_modified = df_modified.drop(columns=['Name','Group','Deck','Total_Gasto'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eliminamos Name y nos quedamos con LastName que es el que aporta información\n",
        "\n",
        "Eliminamos Group porque ya no nos sirve y nos quedamos con PassengerId\n",
        "\n",
        "Eliminamos Cabin porque hemos visto que lo que nos interesa es Desck\n",
        "\n",
        "Eliminamos las columnas de gastos y nos quedamos con el total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts2xeUavQcwK"
      },
      "outputs": [],
      "source": [
        "# Identificar columnas con un solo valor único\n",
        "constant_columns = [col for col in df_modified.columns if df_modified[col].nunique() == 1]\n",
        "print(f\"\\n{Fore.CYAN}Columnas que tienen un único valor en todas sus filas:\\n{Fore.RESET}\")\n",
        "print(constant_columns)\n",
        "\n",
        "# Opcional: Si necesitas que ocurra algo si no hay columnas constantes, puedes poner el bloque del if\n",
        "if constant_columns == []:\n",
        "    print(\"No hay columnas con valores constantes.\")\n",
        "\n",
        "# Hacemos una copia del dataset\n",
        "df_dropped = df_modified.copy()\n",
        "\n",
        "# Eliminar las columnas que tienen un único valor en todas sus filas\n",
        "df_dropped = df_dropped.drop(columns=constant_columns)\n",
        "\n",
        "# Verificar las columnas restantes\n",
        "print(f\"\\n{Fore.CYAN}Verificamos las columnas restantes:\\n{Fore.RESET}\")\n",
        "print(df_dropped.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def imputar_lastname_con_letras(df):\n",
        "    # Paso 1: Filtrar los valores nulos en LastName\n",
        "    mask_null_lastname = df['LastName'].isnull()\n",
        "    num_nulos = mask_null_lastname.sum()  # Contar cuántos valores nulos hay\n",
        "\n",
        "    # Paso 2: Generar combinaciones únicas de letras aleatorias\n",
        "    letras_usadas = set(df['LastName'].dropna())  # Asegurar que no se repiten con los existentes\n",
        "    nuevos_lastnames = set()\n",
        "\n",
        "    # Generar nuevas combinaciones de 3 letras aleatorias hasta tener suficientes nombres únicos\n",
        "    while len(nuevos_lastnames) < num_nulos:\n",
        "        new_lastname = 'Lastname'.join(random.choices(string.ascii_uppercase, k=3))\n",
        "        if new_lastname not in letras_usadas and new_lastname not in nuevos_lastnames:\n",
        "            nuevos_lastnames.add(new_lastname)\n",
        "\n",
        "    # Paso 3: Convertir a lista para asignar\n",
        "    nuevos_lastnames = list(nuevos_lastnames)\n",
        "\n",
        "    # Imputar los valores nulos con las nuevas combinaciones generadas\n",
        "    df.loc[mask_null_lastname, 'LastName'] = nuevos_lastnames\n",
        "\n",
        "    print(f\"Se han imputado {len(nuevos_lastnames)} valores únicos en la columna LastName.\")\n",
        "    return df\n",
        "\n",
        "# Aplicar la función de imputación\n",
        "df_modified = imputar_lastname_con_letras(df_modified)\n",
        "\n",
        "# Verificar que no queden valores nulos\n",
        "print(f\"Valores nulos en LastName después de la imputación: {df_modified['LastName'].isnull().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar los nulos por variable.\n",
        "print(f\"\\n{Fore.CYAN}Valores nulos por columna:\\n{Fore.RESET}\")\n",
        "df_modified.isnull().sum()  # Suma los valores nulos por columna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 732,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_modified = df_modified.dropna(subset=['Cabin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar el tamaño del gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Crear un heatmap para mostrar los valores nulos\n",
        "sns.heatmap(df_modified.isnull(), cbar=False, cmap='viridis')\n",
        "\n",
        "# Título del gráfico\n",
        "plt.title('Visualización de Valores Nulos en el Dataset')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Guardar el heatmap y limpiar la figura\n",
        "save_and_clear_plot(\"heatmap_valores_nulos.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Renombramos las columnas\n",
        "df = df_modified.rename(columns={\n",
        "    # 'Deck': 'Cabin',\n",
        "    'RoomService_log': 'RoomService',\n",
        "    'ShoppingMall_log': 'ShoppingMall',\n",
        "    'FoodCourt_log': 'FoodCourt',\n",
        "    'Spa_log': 'Spa',\n",
        "    'VRDeck_log': 'VRDeck',\n",
        "    'LastName': 'Name'\n",
        "})\n",
        "\n",
        "# Luego reordenamos las columnas en el orden original\n",
        "new_order = ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', \n",
        "             'RoomService', 'ShoppingMall', 'FoodCourt', 'Spa', 'VRDeck', 'Name', 'Transported']\n",
        "\n",
        "df = df[new_order]\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el dataset modificado en un archivo CSV\n",
        "df.to_csv('../data/df_model_test.csv', index=False)\n",
        "\n",
        "# Verificar que el archivo se ha guardado correctamente\n",
        "print(\"El dataset ha sido guardado como 'df_model_test.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
